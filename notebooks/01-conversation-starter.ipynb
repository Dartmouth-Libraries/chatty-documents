{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversation starter\n",
    "\n",
    "This notebook goes over the basics of interacting with a Large Language Model using the Python library [LangChain](https://www.langchain.com/). Most of the example code here is adapted from the official [LangChain docs](https://python.langchain.com/docs/get_started/quickstart).\n",
    "\n",
    "We will be using the OpenAI API to interact with GPT 3.5. This requires an API key and some additional steps for setup. Make sure you follow the instructions in the [README](README.md#getting-started) to get started. There is also additional information in the introductory [slide deck](../ppt/chatty-documents.pptx).\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Note:** The OpenAI API is a paid service and running this notebook will cause the owner of the API key to get billed! GPT 3.5 costs only fractions of a cent per token (roughly 3/4 of a word), but you still should be mindful not to run unnecessary prompts, especially if they contain a lot of words.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain: A framework for LLM applications\n",
    "\n",
    "_LangChain_ does not provide any Large Language Model by itself. Instead, it is a flexible framework that provides numerous components to build LLM-powered applications. Among other things, _LangChain_ offers a uniform interface to many Large Language Models, even though the LLMs themselves must be provided from elsewhere.\n",
    "\n",
    "For example: If we want to use GPT 3.5 in a _LangChain_ application, we need to install OpenAI's Python library `openai`, as well. When we write the actual code, we only need to interact with _LangChain_ objects, but _LangChain_ calls the `openai` library \"under-the-hood\". This lets us use many LLMs in _LangChain_, so we can easily swap out the LLM in our application with hardly any changes to the code.\n",
    "\n",
    "_LangChain_'s content can be roughly divided in two categories:\n",
    "\n",
    "- **Components**: Useful abstractions for working with LLMs that help us quickly implement common problems\n",
    "- **Chains**: A collection or sequence of components performing a particular task. We can use some pre-defined chains, for common tasks, implement our own, or do a mix of both.\n",
    "\n",
    "This notebook will show you some examples from both of these categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our API key\n",
    "\n",
    "At this point you should have set up a file named `secrets.env` with your OpenAI API key. We will now use a lightweight Python package called `dotenv` to read in this file and set its contents as environment variables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../secrets.env\")\n",
    "\n",
    "os.getenv(\n",
    "    \"OPENAI_API_KEY\"\n",
    ") is not None  # Do not print the key itself! We want to keep it secret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing a Large Language Model\n",
    "\n",
    "There are two types of language models, which in LangChain are called:\n",
    "\n",
    "- LLMs: this is a language model which takes a single string as input and returns a single string\n",
    "- ChatModels: this is a language model which takes a list of messages as input and returns a message\n",
    "\n",
    "We use separate objects for each kind of model. Here, we will create an instance of each kind of model from the [GPT 3.5 family of models](https://platform.openai.com/docs/models/gpt-3-5):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! _LangChain_ takes care of all everything else for us behind the scenes: Authenticating us using the API key, routing our requests to the right endpoints, retrieving and parsing the response. In the next section, we will take a look at how we can send a prompt to these models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic prompting using strings\n",
    "\n",
    "The most straightforward way to send a prompt to the model is to use a string and `predict()` method, which is implemented by bothmodels:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm.predict(\"What is a Large Language Model?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    chat_model.predict(\n",
    "        \"What is the difference between a Large Language Model and a Chat Model?\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the chat model is essentially state-less and does not have any \"memory\" of the conversation in this case:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat_model.predict(\"Tell me more about that.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting with `Message`s\n",
    "\n",
    "An alternative to using a plain string to prompt a model is to use a `Message` object. Such objects contain the actual prompt, but they also contain a role descriptor in their `type` property:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "\n",
    "text = \"What would be a good name for my dog?\"\n",
    "\n",
    "message = HumanMessage(content=text)\n",
    "message.content, message.type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can prompt an LLM with a list of messages using the `predict_messages()` method. In that case, the model returns another `Message` object in response:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.predict_messages([message])\n",
    "print(response.type)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_model.predict_messages([message])\n",
    "print(response.type)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using a chat model, you can now continue the conversation by just keeping a running list of the messages (both human and AI):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [message, response]\n",
    "new_prompt = HumanMessage(\n",
    "    content=\"Can you tell me a word that rhymes with the last name in that list?\"\n",
    ")\n",
    "conversation.append(new_prompt)\n",
    "print(chat_model.predict_messages(conversation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Keeping track of the conversation manually can become rather tedious. _LangChain_ therefore offers extensive support to streamline this process (called [Memory](https://python.langchain.com/docs/modules/memory/)), but it is beyond the scope of this session.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt templates\n",
    "\n",
    "Large Language Model applications often use very similar prompts over and over again with only minor changes to their content.\n",
    "\n",
    "Continuing the previous example, we might want to ask for names for all of our different pets. In that case, we would ask the same question repeatedly, only changing the kind of pet we are asking about.\n",
    "\n",
    "For such use cases, _LangChain_ offers the `PromptTemplate` object. This object allows us to create a template with a placeholder that we can replace with whatever string we like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"What is a good name for my {pet}?\")\n",
    "\n",
    "prompt.format(pet=\"lizard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have such a template, we can iterate over a list of different kinds of pets and ask for name suggestions with very little code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pets = [\"dog\", \"cat\", \"lizard\", \"goldfish\"]\n",
    "for pet in pets:\n",
    "    print(\"*\" * 10 + pet + \"*\" * 10)\n",
    "    print(llm.predict(prompt.format(pet=pet)))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pet in pets:\n",
    "    print(\"*\" * 10 + pet + \"*\" * 10)\n",
    "    print(chat_model.predict(prompt.format(pet=pet)))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more advanced version of the prompt template is based on `Message`s. In this `ChatPromptTemplate`, we can combine our regular prompt with a special system message that instructs the model how to behave in the conversation.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "There is nothing truly special about the system message per se: It is also just a string. However, it is marked as \"system\", as opposed to \"ai\" or \"human\", and language models can be tuned to pay particular attention to this message to adapt their output style.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "system_template = \"Respond to every prompt as if you were a {persona}.\"\n",
    "human_template = \"What is a good name for my {pet}?\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_template),\n",
    "        (\"human\", human_template),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_prompt.format_messages(persona=\"cat\", pet=\"dog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this template to ask for more pet names:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pet in pets:\n",
    "    print(\"*\" * 10 + pet + \"*\" * 10)\n",
    "    print(\n",
    "        llm.predict_messages(\n",
    "            chat_prompt.format_messages(persona=\"cat\", pet=pet)\n",
    "        ).content\n",
    "    )\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pet in pets:\n",
    "    print(\"*\" * 10 + f\" {pet} \" + \"*\" * 10)\n",
    "    print(\n",
    "        chat_model.predict_messages(\n",
    "            chat_prompt.format_messages(persona=\"cat\", pet=pet)\n",
    "        ).content\n",
    "    )\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting structured output for further processing\n",
    "\n",
    "Passing messages back and forth makes sense for a chat-like application, but often we want the model to do some sort of analysis and return structured results. To accomplish that, we can ask the LLM to provide the output data in a certain format and then pass the response through a matching parser.\n",
    "\n",
    "We could use any parser or parsing library we like, but _LangChain_ comes with a parser base object that plays nicely with the other components, and that we can customize to fit our needs.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "If you are not familiar with the concept of _inheritance_ or _subclassing_, you can check out our workshop on [Object-Oriented Programming](https://git.dartmouth.edu/lib-digital-strategies/RDS/workshops/computational-tools/classy-code), which may be helpful to fully understand the next section.\n",
    "\n",
    "</div>\n",
    "\n",
    "To get the pet names as a comma-separated list, for example, we would create a coresponding parser like so:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "\n",
    "class CommaSeparatedListOutputParser(BaseOutputParser):\n",
    "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        return text.strip().split(\", \")\n",
    "\n",
    "\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "parser.parse(\"One, two\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can pass the response from the model to the parser:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"\"\"You are a helpful assistant who generates comma separated lists. You ONLY return the comma separated lists, nothing else.\"\"\"\n",
    "human_template = \"List 5 good names for my {pet}?\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_template),\n",
    "        (\"human\", human_template),\n",
    "    ]\n",
    ")\n",
    "for pet in pets:\n",
    "    print(\n",
    "        parser.parse(\n",
    "            chat_model.predict_messages(chat_prompt.format_messages(pet=pet)).content\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "By now, we have created several components that together make up a kind of processing pipeline:\n",
    "\n",
    "- Use a specified prompt template\n",
    "- Feed it to a specified LLM\n",
    "- Parse the LLM's output with a specific parser\n",
    "\n",
    "These steps can be combined into a so-called _chain_. To that end, _LangChain_ defines a declarative way of expressing this concatenation using the [LangChain Expression Language (LCEL)](https://python.langchain.com/docs/expression_language).\n",
    "\n",
    "Here is how we can define the chain above using the LCEL:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chat_prompt | chat_model | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the chain with the `invoke()` method. This method requires a dictionary that contains all the parameters we need to specify within the chain (e.g., to fill in the gaps in the prompt template) :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pet in pets:\n",
    "    print(chain.invoke({\"pet\": pet}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [next notebook](02-summarizing.ipynb), we will create a chain to do a slightly more meaningful task: Summarize text.\n",
    "\n",
    "<table >\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td style=\"padding:0px;border-width:0px;vertical-align:center\">    \n",
    "    Created by Simon Stone for Dartmouth College Library under <a href=\"https://creativecommons.org/licenses/by/4.0/\">Creative Commons CC BY-NC 4.0 License</a>.<br>For questions, comments, or improvements, email <a href=\"mailto:researchdatahelp@groups.dartmouth.edu\">Research Data Services</a>.\n",
    "    </td>\n",
    "    <td style=\"padding:0 0 0 1em;border-width:0px;vertical-align:center\"><img alt=\"Creative Commons License\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\"/></td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
