{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversation starter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from: [LangChain docs](https://python.langchain.com/docs/get_started/quickstart)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running this outside of [Dartmouth's JupyterHub](https://jhub.dartmouth.edu), make sure you follow the instructions in the [README](README.md#getting-started) to get started.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompting an LLM using LangChain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of language models, which in LangChain are called:\n",
    "\n",
    "- LLMs: this is a language model which takes a string as input and returns a string\n",
    "- ChatModels: this is a language model which takes a list of messages as input and returns a message\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our API key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../secrets.env\")\n",
    "\n",
    "os.getenv(\"OPENAI_API_KEY\") is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic prompting using strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm.predict(\"What is a Large Language Model?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    chat_model.predict(\n",
    "        \"What is the difference between a Large Language Model and a Chat Model?\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting with `Message`s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"What would be a good name for my dog?\"\n",
    "messages = [HumanMessage(content=text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm.predict_messages(messages).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat_model.predict_messages(messages).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt templates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"What is a good name for my {pet}?\")\n",
    "\n",
    "prompt.format(pet=\"lizard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pets = [\"dog\", \"cat\", \"lizard\", \"goldfish\"]\n",
    "for pet in pets:\n",
    "    print(\"*\" * 10 + pet + \"*\" * 10)\n",
    "    print(llm.predict(prompt.format(pet=pet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pet in pets:\n",
    "    print(\"*\" * 10 + pet + \"*\" * 10)\n",
    "    print(chat_model.predict(prompt.format(pet=pet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "system_template = \"Respond to every prompt as if you were a {pet}.\"\n",
    "human_template = \"What is a good name for my {pet}?\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_template),\n",
    "        (\"human\", human_template),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_prompt.format_messages(pet=\"dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pet in pets:\n",
    "    print(\"*\" * 10 + pet + \"*\" * 10)\n",
    "    print(llm.predict_messages(chat_prompt.format_messages(pet=pet)).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pet in pets:\n",
    "    print(\"*\" * 10 + f\" {pet} \" + \"*\" * 10)\n",
    "    print(chat_model.predict_messages(chat_prompt.format_messages(pet=pet)).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting structured output for further processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "\n",
    "class CommaSeparatedListOutputParser(BaseOutputParser):\n",
    "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        return text.strip().split(\", \")\n",
    "\n",
    "\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "parser.parse(\"One, two\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"\"\"You are a helpful assistant who generates comma separated lists. You ONLY return the comma separated lists, nothing else.\"\"\"\n",
    "human_template = \"List 5 good names for my {pet}?\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_template),\n",
    "        (\"human\", human_template),\n",
    "    ]\n",
    ")\n",
    "for pet in pets:\n",
    "    print(\n",
    "        parser.parse(\n",
    "            chat_model.predict_messages(chat_prompt.format_messages(pet=pet)).content\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Creating a chain using the [LangChain Expression Language](https://python.langchain.com/docs/expression_language).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chat_prompt | chat_model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pet in pets:\n",
    "    print(chain.invoke({\"pet\": pet}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
