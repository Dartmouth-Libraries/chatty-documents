{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizing texts\n",
    "\n",
    "Large Language Models are very good at processing large amounts of text and paying attention to the most important parts. We can use this ability to summarize texts.\n",
    "\n",
    "In this notebook, we will use an LLM to summarize the [Federalist Papers](https://en.wikipedia.org/wiki/The_Federalist_Papers).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our API key\n",
    "\n",
    "At this point you should have set up a file named `secrets.env` with your OpenAI API key. We will now use a lightweight Python package called `dotenv` to read in this file and set its contents as environment variables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../secrets.env\")\n",
    "\n",
    "os.getenv(\n",
    "    \"OPENAI_API_KEY\"\n",
    ") is not None  # Do not print the key itself! We want to keep it secret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading text documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running this on Dartmouth's JupyterHub, the documents are already stored as individual text files for you as a dataset online. If you are running this anywhere else, [download and extract the dataset](https://git.dartmouth.edu/lib-digital-strategies/RDS/datasets/federalist-papers-dataset/-/archive/main/federalist-papers-dataset-main.zip) and change the path in the next cell accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "docs_dir = Path(\n",
    "    \"~/shared/RR-workshop-data/federalist-papers-dataset/split\"\n",
    ").expanduser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use regular Python to read each of the papers like so:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(docs_dir / \"federalist_1.txt\") as f:\n",
    "    doc = f.read()\n",
    "\n",
    "print(doc[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can benefit from _LangChain_'s document loaders, which reads all the files in the directory with just a few lines of code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "doc_loader = DirectoryLoader(docs_dir, show_progress=True)\n",
    "docs = doc_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a summary\n",
    "\n",
    "### Stuffing\n",
    "\n",
    "The most straightforward way to ask a model to summarize a piece of text, is to send the entire text and ask the LLM to summarize it. This approach is called _stuffing_:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from openai.error import InvalidRequestError\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "try:\n",
    "    for doc in docs[:3]:\n",
    "        print(\"-\" * 30)\n",
    "        print(\n",
    "            llm.predict(\n",
    "                \"Summarize the following text in 200 words: \\n\" + doc.page_content\n",
    "            )\n",
    "        )\n",
    "except InvalidRequestError as e:\n",
    "    print(e._message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the model successfully summarizes the first document, but fails to summarize the second one.\n",
    "\n",
    "The reason for this is the length of the text: A Large Language Model cannot process arbitrarily long input strings. The maximum number of tokens (one token ~ 3/4 word) an LLM can take into account while producing the next output token is called the _context window_. You can think of this as the \"attention span\" of the model.\n",
    "\n",
    "The context window for the [regular GPT 3.5 turbo is 4097](https://platform.openai.com/docs/models/gpt-3-5), but there is model called `'gpt-3.5-turbo-16k'` that offers four times the context window. So let's switch to that and try again:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\")\n",
    "\n",
    "for doc in docs[:3]:\n",
    "    print(\"-\" * 30)\n",
    "    print(\n",
    "        llm.predict(\"Summarize the following text in 200 words: \\n\" + doc.page_content)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a chain\n",
    "\n",
    "Since summarizing documents is a popular use-case, _LangChain_ offers a pre-configured chain `StuffDocumentsChain`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "\n",
    "# Define prompt\n",
    "prompt_template = \"Write a concise summary of the following:{text}\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# Define LLM chain\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-16k\")\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Define StuffDocumentsChain\n",
    "stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"text\")\n",
    "\n",
    "print(stuff_chain.run(docs[:1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convenience wrapper\n",
    "\n",
    "We can simplify this even more by using the convenient wrapper `load_summarize_chain()`, which comes with a predefined prompt suitable for this task:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "chain = load_summarize_chain(llm=llm, chain_type=\"stuff\")\n",
    "\n",
    "chain.run(docs[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map-reduce\n",
    "\n",
    "Stuffing works great for shorter texts and models with a large context window. But what if we want to process longer documents, or a much larger number of documents?\n",
    "\n",
    "One technique to consider in this case is _Map-reduce_. It consists of two steps:\n",
    "\n",
    "1. **Map**: Chunk the texts to summarize into smaller units (e.g., shorter documents, paragraphs) and summarize each chunk\n",
    "2. **Reduce**: Treat the summaries like new documents and summarize the summaries with the LLM, using either Stuffing or another iteration of Map-Reduce\n",
    "\n",
    "Just like with Stuffing, _LangChain_ also supports Map-reduce with specialized objects that facilitate the process.\n",
    "\n",
    "We split the total map-reduce chain into a map chain and a reduce chain. These chains consist of a model and a prompt:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map\n",
    "map_template = \"Write a concise summary of the following: {docs}\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
    "\n",
    "# Reduce\n",
    "reduce_template = \"Distill the following summaries into a single summary of the main topics: {doc_summaries}\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not guaranteed that the sum of all the outputs of the _map_ chain fit into the context window of the _reduce_ chain. We therefore may have to run _map-reduce_ iteratively. We could implement that manually, but _LangChain_ offers some convenient components to help with this.\n",
    "\n",
    "In addition to these LLm chains, we need two components:\n",
    "\n",
    "1. An object that handles the stuffing part preceding the _map_ step\n",
    "2. An object that handles the reduce step\n",
    "\n",
    "We can use _LangChain_'s `StuffDocumentsChain` to do the first task:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain,\n",
    "    document_variable_name=\"doc_summaries\",  # This is the name of the prompt variable to fill with the stuffed documents\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second part can be handled by a `ReduceDocumentsChain`. Since it may have to run multiple iterations, this object needs to have a way to combine the input documents, in case they do not fit into the context window as one long string. This is defined by the object we defined in the previous step:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ReduceDocumentsChain\n",
    "\n",
    "# Combines and iteravely reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is how the documents get combined\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # This is the chain to use to deal with documents that do not fit into the context window\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can put it all together in a `MapReduceDocumentsChain`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import MapReduceDocumentsChain\n",
    "\n",
    "# Combining documents by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run this chain for the first 10 documents:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(map_reduce_chain.run(docs[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like a decent summary of the main topics!\n",
    "\n",
    "In the [next notebook](03-document-qa.ipynb), we will go one step further and ask the LLM more specific questions related to a specific document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table >\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td style=\"padding:0px;border-width:0px;vertical-align:center\">    \n",
    "    Created by Simon Stone for Dartmouth College Library under <a href=\"https://creativecommons.org/licenses/by/4.0/\">Creative Commons CC BY-NC 4.0 License</a>.<br>For questions, comments, or improvements, email <a href=\"mailto:researchdatahelp@groups.dartmouth.edu\">Research Data Services</a>.\n",
    "    </td>\n",
    "    <td style=\"padding:0 0 0 1em;border-width:0px;vertical-align:center\"><img alt=\"Creative Commons License\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\"/></td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
